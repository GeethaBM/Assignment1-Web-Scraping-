{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeethaBM/Assignment1-Web-Scraping-/blob/main/assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9 Write a program to scrap all the available details of best gaming laptops from digit.in."
      ],
      "metadata": {
        "id": "5UhhhVeAgZUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAstabLRClow",
        "outputId": "4d59d847-4b08-499d-c03e-dc7d321542ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.3.0-py3-none-any.whl (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 8.3 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.21.0-py3-none-any.whl (358 kB)\n",
            "\u001b[K     |████████████████████████████████| 358 kB 54.1 MB/s \n",
            "\u001b[?25hCollecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 57.0 MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Collecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting cryptography>=1.3.4\n",
            "  Downloading cryptography-37.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 44.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.6.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.11 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-37.0.4 h11-0.13.0 outcome-1.2.0 pyOpenSSL-22.0.0 selenium-4.3.0 sniffio-1.2.0 trio-0.21.0 trio-websocket-0.9.2 urllib3-1.26.11 wsproto-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import csv"
      ],
      "metadata": {
        "id": "pAAlotH2D4pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Flip_scrap():\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "        self.current_path = os.getcwd()\n",
        "        self.url = 'https://www.digit.in.'\n",
        "        \n",
        "        self.driver_path = os.path.join(os.getcwd(), 'chromedriver')\n",
        "        self.driver = webdriver.Chrome(self.driver_path)\n",
        "\n",
        "    def flip_load(self):\n",
        "\n",
        "        self.driver.get(self.url)\n",
        "        try:\n",
        "            popup_close = self.driver.find_element_by_class_name('_29YdH8')\n",
        "            popup_close.click()\n",
        "            print('pop-up closed')\n",
        "        except:\n",
        "            pass\n",
        "      \n",
        "        search_query = self.driver.find_element_by_class_name('LM6RPg')\n",
        "        search_query.send_keys('gaming laptop'  + '\\n')\n",
        "        time.sleep(2)\n",
        "        page_html = self.driver.page_source\n",
        "        self.soup = BeautifulSoup(page_html, 'html.parser')\n",
        "\n",
        "    def file_create(self):\n",
        "\n",
        "\n",
        "        headings = [\"Name\", \"Storage_details\", \"Screen_size\", \"Operating_system\", \"RAM\", \"Processor\", \"Warranty\", \"Price in Rupees\"]\n",
        "        self.file_csv = open('gaming_laptop_output.csv', 'w', newline='', encoding='utf-8')\n",
        "        self.mycsv = csv.DictWriter(self.file_csv, fieldnames=headings)\n",
        "        self.mycsv.writeheader()\n",
        "\n",
        "    def scrap_web(self):\n",
        "\n",
        "        for j in range (9):\n",
        "            \n",
        "            time.sleep(1)\n",
        "            gaming_laptop = (self.soup.find_all('div', class_='_3O0U0u'))\n",
        "            for i in gaming_laptop:\n",
        "                Name = i.find('img', class_='_1Nyybr')['alt']\n",
        "                price = i.find('div', class_='_1vC4OE _2rQ-NK')\n",
        "                details = i.find_all(\"li\")\n",
        "                processor = details[0].text\n",
        "                ram = details[1].text\n",
        "                opos = details[2].text\n",
        "                storagesize = details[3].text\n",
        "                screensize = details[4].text\n",
        "                warrantydet= details[5].text\n",
        "\n",
        "                price = price.text[1:]\n",
        "                if 'cm' in screensize:\n",
        "                    self.mycsv.writerow({\"Name\": Name, \"Storage_details\": storagesize, \"Screen_size\": screensize, \"Operating_system\": opos, \"RAM\": ram, \"Processor\": processor, \"Warranty\": warrantydet, \"Price in Rupees\": price})\n",
        "                \n",
        "\n",
        "            next_query = self.driver.find_element_by_xpath(\"//span[text()='Next']\")\n",
        "            next_query.click()\n",
        "            time.sleep(2)\n",
        "\n",
        "    def quitprocess(self):\n",
        "\n",
        "        self.driver.quit()\n",
        "        self.file_csv.close()\n"
      ],
      "metadata": {
        "id": "qsvFCwu0fOp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    Flip_scrap = Flip_scrap()\n",
        "    Flip_scrap.flip_load()\n",
        "    Flip_scrap.file_create()\n",
        "    Flip_scrap.scrap_web()\n",
        "    Flip_scrap.quitprocess()\n",
        "  \n",
        "    print(\"Mission Accomplished\")"
      ],
      "metadata": {
        "id": "May3k59pfBfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
      ],
      "metadata": {
        "id": "bJmINCUZkPC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "headers = {\n",
        "    \"accept\": \"application/json, text/plain, */*\",\n",
        "    \"referer\": \"https://www.forbes.com/global2000/\",\n",
        "    \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.67 Safari/537.36\",\n",
        "}\n",
        "\n",
        "cookies = {\n",
        "    \"notice_behavior\": \"expressed,eu\",\n",
        "    \"notice_gdpr_prefs\": \"0,1,2:3KniJQ6YQyNAp3UW6ggYsoCtWjk9FyjUUC\",\n",
        "}\n",
        "\n",
        "api_url = \"https://www.forbes.com/forbesapi/org/global2000/2020/position/true.json?limit=2000\"\n",
        "response = requests.get(api_url, headers=headers, cookies=cookies).json()\n",
        "\n",
        "sample_table = [\n",
        "    [\n",
        "        item[\"organizationName\"],\n",
        "        item[\"country\"],\n",
        "        item[\"revenue\"],\n",
        "        item[\"profits\"],\n",
        "        item[\"assets\"],\n",
        "        item[\"marketValue\"]\n",
        "    ] for item in\n",
        "    sorted(response[\"organizationList\"][\"organizationsLists\"], key=lambda k: k[\"position\"])\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(sample_table, columns=[\"Company\", \"Country\", \"Sales\", \"Profits\", \"Assets\", \"Market Value\"])\n",
        "df.to_csv(\"forbes_2020.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yr3GMivfElN",
        "outputId": "5dedcf8d-d420-457f-a02b-790022d6d0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9 Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
      ],
      "metadata": {
        "id": "VC49ydW3l_sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "check below link for 9 question answer and just copy all from that it's run fine with you"
      ],
      "metadata": {
        "id": "lf9IgXD1mZsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "https://medium.com/analytics-vidhya/extracting-youtube-comments-using-selenium-b29ee4f743ef"
      ],
      "metadata": {
        "id": "wHBjwL8Qkm-3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}